# 13 - 会话压缩需求

> **版本**: v1.0
> **最后更新**: 2026-01-29
> **依赖文档**: 12-会话管理需求
> **后续文档**: 14-Agent基础架构需求

---

## 1. 需求概述

### 1.1 目标描述

实现**上下文窗口管理和自动压缩**，当对话历史超过模型的上下文窗口限制时，自动总结和压缩旧消息，保留关键信息。

**核心目标:**
- **Token 计数**: 实时统计上下文 Token 使用
- **窗口守卫**: 监控 Token 使用，接近限制时触发压缩
- **渐进式压缩**: 逐步压缩，而非一次性压缩
- **关键信息保留**: 保留重要的上下文信息

### 1.2 业务场景

#### 场景 1: 长对话压缩
**触发**: 用户与 Agent 长时间对话，历史消息超过上下文窗口。

**流程**:
```
1. 检测 Token 使用超过阈值（如 80%）
2. 触发压缩
3. 总结前 N 条消息
4. 替换原消息为总结
5. 继续 Agent 运行
```

#### 场景 2: 压缩策略
**配置**:
```json5
{
  session: {
    compaction: {
      enabled: true,
      strategy: "sliding",      // 滑动窗口
      targetRatio: 0.5,         // 压缩到 50%
      minMessages: 10           // 至少保留 10 条消息
    }
  }
}
```

---

## 2. 核心概念

### 2.1 上下文窗口

**定义**: 模型支持的最大输入 Token 数。

**常见限制**:
- Claude Sonnet: 200K tokens
- GPT-4: 128K tokens
- Gemini Pro: 1M tokens

### 2.2 压缩策略

#### 策略 1: 滑动窗口（Sliding）
**说明**: 保留最近的 N 条消息，总结并压缩更早的消息。

#### 策略 2: 重要性采样（Importance）
**说明**: 根据消息重要性选择性保留。

### 2.3 Token 计数

**方法**:
- 使用 tiktoken 库估算 Token 数
- 不同模型使用不同的 tokenizer

---

## 3. 关键实现

### 3.1 实现步骤

#### 步骤 1: Token 计数

```typescript
import { encodingForModel } from 'tiktoken';

export function countTokens(text: string, model: string): number {
  const encoding = encodingForModel(model);
  const tokens = encoding.encode(text);
  return tokens.length;
}
```

#### 步骤 2: 压缩触发

```typescript
export async function compactSessionIfNeeded(
  sessionId: string,
  contextLimit: number,
  currentTokens: number
) {
  const threshold = contextLimit * 0.8; // 80% 阈值

  if (currentTokens > threshold) {
    await compactSession(sessionId, contextLimit * 0.5); // 压缩到 50%
  }
}
```

#### 步骤 3: 总结消息

```typescript
export async function summarizeMessages(
  messages: Message[]
): Promise<string> {
  // 调用 AI 模型总结消息
  const prompt = `总结以下对话，保留关键信息：\n${formatMessages(messages)}`;
  const summary = await callLLM(prompt);
  return summary;
}
```

---

**文档完成** ✅
